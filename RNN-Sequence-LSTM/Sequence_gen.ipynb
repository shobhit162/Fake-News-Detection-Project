{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_gen.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvwDGQxn_UBR"
      },
      "source": [
        "Use of Bidirectional_LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfwnPDCJaRGE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZBoIZqRjvsO"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikZnr45vctRN"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding , LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.utils as k\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Faju4bfogRxA"
      },
      "source": [
        "token = Tokenizer()\n",
        "data = open('i.txt').read()\n",
        "corpus = data.lower().split('\\n')\n",
        "token.fit_on_texts(corpus)\n",
        "total_words = len(token.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kre1UCiHjowX"
      },
      "source": [
        "input_seq =[]\n",
        "for line in corpus:\n",
        "    token_list = token.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequences = token_list[:i+1]\n",
        "        input_seq.append(n_gram_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX86gzKXmuOt"
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_seq])\n",
        "input_sequences = np.array(pad_sequences(input_seq, maxlen=max_sequence_len , padding='pre'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlYGPEH4n-cw"
      },
      "source": [
        "X_train , Y_train = input_sequences[: , :-1] , input_sequences[: , -1]\n",
        "Y_train = k.to_categorical(Y_train, num_classes = total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5VZTobBqQl5"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words ,1000 , input_length = max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words / 2 , activation='relu'))\n",
        "model.add(Dense(total_words , activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1I1SuFStVq6"
      },
      "source": [
        "history = model.fit(X_train , Y_train , epochs  = 15  , verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQOc0_O8vEvL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs , acc , 'b' , label = 'Traning_accuracy')\n",
        "plt.title('Tranning_accuracy')\n",
        "plt.figure()\n",
        "plt.plot(epochs , loss , 'b' , label = 'Traning_accuracy')\n",
        "plt.title('Traning_loss')\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8hHsyYu83H1"
      },
      "source": [
        "text = 'you know me well'\n",
        "next_word = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REY5QjnX83Dx"
      },
      "source": [
        "for _ in range(next_word):\n",
        "    token_list = token.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen = max_sequence_len , padding = 'pre')\n",
        "    predicted = model.predict_classes(token_list , verbose = 0)\n",
        "    for word , index in token.word_index.items():\n",
        "        if index == predicted:\n",
        "            output = word\n",
        "            break\n",
        "\n",
        "    text += ' '+output\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVaO8uEx-EpA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}